In several domains, multi-gigapixel images must be analysed for the purpose of gathering information and/or for taking decisions. Typically, the information is represented by the presence of a series of objects of interest which are embedded into the image. The goal of the analyse is to locate and identify those objects. Depending on the problem and  specific field of application, the extracted objects can be used for various purposes. For instance, in cytology, digitized microscope slides containing human tissues are analysed by physicians in order to diagnose particular disease, the disease in question manifesting itself by the presence of cells having certain characteristics. In geology, slides containing core samples can be digitized and analysed to find concentration of certain micro-organisms.

Those images are usually analysed manually by experts. However, due to the size of the problem, the analyse is not always exhaustive. When possible, experts typically select a reduced number areas to study and draw conclusions from the observations performed in those areas. This process has obviously the drawback of increasing the risk of missing objects of interest.

Because of the risk yielded by the previous method and because manual analysis of full images is long and tedious, computer programs could be used to assist the experts. For instance, those programs could indicate which regions are worth analysing and which are not. They could also perform the search in place of the expert but under his supervision: that is, the expert would be able to provide a feedback to the program which could then improve its detection process. 

Such programs typically use image processing (IP) techniques to provide this assistance and, in order to learn from experts' feedback, machine learning (ML) is also used. Whereas IP and ML provide a complete toolbox of algorithms for computer vision in general, they are however not necessarily well suited for handling large images. Especially, typical implementations of those algorithms implicitly make the assumption that the full image can be loaded into memory which is not always possible. Indeed, multi-gigapixel images typically requires several gigabytes of memory. As far as the execution times of those algorithms are concerned, they generally grow with the size of the image, yielding unacceptable execution times in sequential execution. Parallelism can alleviate this problem but, again, typical implementations does not necessarily support this feature. Therefore, when diving into a new problem of object detection, implementers typically develop workflows by combining machine learning and image processing algorithms to handle detection but they also have to deal with problem independent concerns such as parallelism or memory constraints. 

In this thesis is proposed \textit{SLDC}, a generic framework for solving problems of object detection and classification in large images. Especially, it provides to implementers a structure to define problem dependent components of the algorithm: that is, the detection and classification. Every other concerns such as parallelization and large image handling are encapsulated by the framework. The framework also provides a way to execute several processing workflow one after another on the same image and a powerful logging system. 

In Chapter \ref{chap:context}, the problem of object detection and classification is introduced and its application to different cases is presented. In Chapter \ref{chap:work_intro}, the framework and its implementation are presented. In Chapter \ref{chap:thyroid}, the framework is applied to a cytology problem, the thyroid case.